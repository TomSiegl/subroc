{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Plots of Metrics Values on search Data vs. holdout-generalizability Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Values for Papermill Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "PARAM_FULL_RESULT_SET_PATH = \"../outputs/p_value_augmented_result_set.csv\"\n",
    "PARAM_FILTERED_RESULT_SET_PATH = \"../outputs/p_value_filtered_result_set.csv\"\n",
    "PARAM_QF_PATH = \"../outputs/interestingness_measure.pickle\"\n",
    "PARAM_PLOT_BASENAME = \"generalizability_plot\"\n",
    "PARAM_DATA_IN_PATH = \"../../data\"\n",
    "PARAM_MODELS_IN_PATH = \"../../models\"\n",
    "\n",
    "PARAM_DATASET_NAME = \"OpenML Adult\"\n",
    "PARAM_DATASET_STAGE = None\n",
    "PARAM_MODEL_NAME = \"sklearn_gaussian_nb_adult_4_splits\"\n",
    "\n",
    "PARAM_PLOT_XMIN = 0\n",
    "PARAM_PLOT_XMAX = 1\n",
    "PARAM_PLOT_YMIN = 0\n",
    "PARAM_PLOT_YMAX = 1\n",
    "PARAM_PLOT_XLABEL = \"PRC AUC on Search Data\"\n",
    "PARAM_PLOT_YLABEL = \"PRC AUC on Test Data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subroc.datasets.metadata import to_DatasetName\n",
    "from subroc.datasets.reader import DatasetReader, DatasetStage, meta_dict\n",
    "from subroc.model_serialization import deserialize\n",
    "from subroc.quality_functions.base_qf import PredictionType\n",
    "from subroc.quality_functions.soft_classifier_target import SoftClassifierTarget\n",
    "from subroc import util\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pysubgroup as ps\n",
    "\n",
    "# fill environment variables into params\n",
    "PARAM_FULL_RESULT_SET_PATH = util.prepend_experiment_output_path(PARAM_FULL_RESULT_SET_PATH)\n",
    "PARAM_FILTERED_RESULT_SET_PATH = util.prepend_experiment_output_path(PARAM_FILTERED_RESULT_SET_PATH)\n",
    "PARAM_QF_PATH = util.prepend_experiment_output_path(PARAM_QF_PATH)\n",
    "PARAM_DATA_IN_PATH = util.prepend_experiment_output_path(PARAM_DATA_IN_PATH)\n",
    "PARAM_MODELS_IN_PATH = util.prepend_experiment_output_path(PARAM_MODELS_IN_PATH)\n",
    "\n",
    "# get environment variables\n",
    "STAGE_OUTPUT_PATH = os.environ.get(\"STAGE_OUTPUT_PATH\", \"../outputs\")\n",
    "\n",
    "# Dataset\n",
    "dataset_reader = DatasetReader(PARAM_DATA_IN_PATH)\n",
    "\n",
    "DATA_OUT_PATH = f\"{STAGE_OUTPUT_PATH}/data/processed\"\n",
    "if not os.path.exists(DATA_OUT_PATH):\n",
    "    os.makedirs(DATA_OUT_PATH)\n",
    "\n",
    "DATASET_NAME = to_DatasetName(PARAM_DATASET_NAME)\n",
    "\n",
    "if DATASET_NAME is None:\n",
    "    print(f\"dataset name '{PARAM_DATASET_NAME}' not supported.\")\n",
    "\n",
    "if PARAM_DATASET_STAGE is None:\n",
    "    DATASET_STAGE = DatasetStage.PROCESSED_MODEL_READY\n",
    "else:\n",
    "    DATASET_STAGE = DatasetStage(PARAM_DATASET_STAGE)\n",
    "\n",
    "# Model\n",
    "model = deserialize(PARAM_MODELS_IN_PATH, PARAM_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data and preprocess it for the model\n",
    "dataset_meta = meta_dict[DATASET_NAME]\n",
    "\n",
    "# prepare classification predictions\n",
    "dataset_meta.prediction_type = PredictionType.CLASSIFICATION_SOFT\n",
    "\n",
    "search_data = None\n",
    "holdout_generalizability_data = None\n",
    "if DATASET_STAGE == DatasetStage.PROCESSED_MODEL_READY:\n",
    "    search_data = dataset_reader._read_processed(dataset_meta, \"model_ready_test.csv\", \",\")\n",
    "    holdout_generalizability_data = dataset_reader._read_processed(dataset_meta, \"model_ready_holdout_generalizability.csv\", \",\")\n",
    "\n",
    "    search_data_x = search_data.loc[:, search_data.columns != dataset_meta.gt_name]\n",
    "    search_data[dataset_meta.score_name] = model.predict(search_data_x)\n",
    "    holdout_generalizability_data_x = holdout_generalizability_data.loc[:, holdout_generalizability_data.columns != dataset_meta.gt_name]\n",
    "    holdout_generalizability_data[dataset_meta.score_name] = model.predict(holdout_generalizability_data_x)\n",
    "\n",
    "    # save data with predictions\n",
    "    out_path = DATA_OUT_PATH + \"/\" + dataset_meta.dataset_dir\n",
    "    if not os.path.exists(out_path):\n",
    "        os.mkdir(out_path)\n",
    "\n",
    "    search_data.to_csv(out_path + \"/\" + \"model_predicted_test.csv\", index=False)\n",
    "    holdout_generalizability_data.to_csv(out_path + \"/\" + \"model_predicted_holdout_generalizability.csv\", index=False)\n",
    "elif DATASET_STAGE == DatasetStage.PROCESSED_MODEL_PREDICTED:\n",
    "    search_data = dataset_reader._read_processed(dataset_meta, \"model_predicted_test.csv\", \",\")\n",
    "    holdout_generalizability_data = dataset_reader._read_processed(dataset_meta, \"model_predicted_holdout_generalizability.csv\", \",\")\n",
    "elif DATASET_STAGE == DatasetStage.PROCESSED_PERMUTED_MODEL_PREDICTED:\n",
    "    search_data = dataset_reader._read_processed(dataset_meta, \"permuted_model_predicted_test.csv\", \",\")\n",
    "    holdout_generalizability_data = dataset_reader._read_processed(dataset_meta, \"permuted_model_predicted_holdout_generalizability.csv\", \",\")\n",
    "\n",
    "# sd objects\n",
    "target = SoftClassifierTarget(dataset_meta.gt_name, dataset_meta.score_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the Full Result Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_result_set = pd.read_csv(f\"{PARAM_FULL_RESULT_SET_PATH}\")\n",
    "full_result_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the Filtered Result Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_result_set = pd.read_csv(f\"{PARAM_FILTERED_RESULT_SET_PATH}\")\n",
    "filtered_result_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Configure the Interestingness Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_configure_qf(data):\n",
    "    with open(PARAM_QF_PATH, \"rb\") as qf_file:\n",
    "        qf = pickle.load(qf_file)\n",
    "\n",
    "    if isinstance(qf, ps.GeneralizationAwareQF):\n",
    "        qf = qf.qf\n",
    "\n",
    "    # Disable any significance-related changes to the qf value\n",
    "    qf.subgroup_size_weight = 0\n",
    "    qf.subgroup_class_balance_weight = 0\n",
    "    qf.random_sampling_p_value_factor = False\n",
    "    qf.random_sampling_normalization = False\n",
    "\n",
    "    # update the representation of the qf-specific constraints if necessary\n",
    "    if hasattr(qf, \"constraints\"):\n",
    "        for constraint in qf.constraints:\n",
    "            if hasattr(constraint, \"update\"):\n",
    "                constraint.update(data)\n",
    "    \n",
    "    return qf\n",
    "\n",
    "\n",
    "search_qf = read_and_configure_qf(search_data)\n",
    "holdout_generalizability_qf = read_and_configure_qf(holdout_generalizability_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Plot Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metric_value(pattern, qf, data):\n",
    "    # sort data and set up some datastructures to access sorted data\n",
    "    dataset_sorted_by_score = data.sort_values(dataset_meta.score_name)\n",
    "    scores_sorted = dataset_sorted_by_score.loc[:, dataset_meta.score_name]\n",
    "    gt_sorted_by_score = dataset_sorted_by_score.loc[:, dataset_meta.gt_name]\n",
    "    sorted_to_original_index = [index for index, _ in dataset_sorted_by_score.iterrows()]\n",
    "\n",
    "    # recreate the pysubgroup object for the subgroup with a representation for the dataset\n",
    "    sel_conjunction = util.from_str_Conjunction(pattern)\n",
    "    subgroup = util.create_subgroup(data, sel_conjunction.selectors)\n",
    "\n",
    "    # calculate statistics\n",
    "    statistics = qf.calculate_statistics(subgroup, target, data)\n",
    "\n",
    "    # check constraints\n",
    "    if not ps.constraints_satisfied(\n",
    "            qf.constraints,\n",
    "            subgroup,\n",
    "            statistics,\n",
    "            data,\n",
    "    ):\n",
    "        return np.nan\n",
    "    \n",
    "    # get true and predicted labels for subgroup cover\n",
    "    sorted_subgroup_representation = \\\n",
    "        [subgroup.representation[original_index] for original_index in sorted_to_original_index]\n",
    "    sorted_subgroup_y_true = gt_sorted_by_score[sorted_subgroup_representation].to_numpy()\n",
    "    sorted_subgroup_y_pred = scores_sorted[sorted_subgroup_representation].to_numpy()\n",
    "    \n",
    "    # compute the metric values\n",
    "    return qf.metric(sorted_subgroup_y_true, sorted_subgroup_y_pred)\n",
    "\n",
    "\n",
    "full_search_metric_values = []\n",
    "full_holdout_generalizability_metric_values = []\n",
    "\n",
    "for i, result in enumerate(full_result_set.itertuples()):\n",
    "    full_search_metric_values.append(calculate_metric_value(result.pattern, search_qf, search_data))\n",
    "    full_holdout_generalizability_metric_values.append(calculate_metric_value(result.pattern, holdout_generalizability_qf, holdout_generalizability_data))\n",
    "\n",
    "filtered_search_metric_values = []\n",
    "filtered_holdout_generalizability_metric_values = []\n",
    "\n",
    "for i, result in enumerate(filtered_result_set.itertuples()):\n",
    "    filtered_search_metric_values.append(calculate_metric_value(result.pattern, search_qf, search_data))\n",
    "    filtered_holdout_generalizability_metric_values.append(calculate_metric_value(result.pattern, holdout_generalizability_qf, holdout_generalizability_data))\n",
    "\n",
    "search_overall_metric_value = calculate_metric_value(\"Dataset\", search_qf, search_data)\n",
    "holdout_generalizability_overall_metric_value = calculate_metric_value(\"Dataset\", holdout_generalizability_qf, holdout_generalizability_data)\n",
    "\n",
    "print(\"full_search_metric_values:\", full_search_metric_values)\n",
    "print(\"full_holdout_generalizability_metric_values:\", full_holdout_generalizability_metric_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_plot(search_metric_values, holdout_generalizability_metric_values, c, linewidths, filtered):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    only_xs = []\n",
    "\n",
    "    for search_metric_value, holdout_generalizability_metric_value in zip(search_metric_values, holdout_generalizability_metric_values):\n",
    "        if np.isnan(holdout_generalizability_metric_value):\n",
    "            only_xs.append(search_metric_value)\n",
    "            continue\n",
    "            \n",
    "        xs.append(search_metric_value)\n",
    "        ys.append(holdout_generalizability_metric_value)\n",
    "\n",
    "    plt.scatter(search_metric_values, holdout_generalizability_metric_values, s=30, c=c, marker=\"x\", linewidths=linewidths)\n",
    "\n",
    "    for x in only_xs:\n",
    "        if filtered:\n",
    "            plt.axvline(x, color=\"black\", linestyle=\"--\", linewidth=0.5)\n",
    "        else:\n",
    "            plt.axvline(x, color=\"gray\", linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "scatter_plot(full_search_metric_values, full_holdout_generalizability_metric_values, c=\"gray\", linewidths=0.5, filtered=False)\n",
    "scatter_plot(filtered_search_metric_values, filtered_holdout_generalizability_metric_values, c=\"black\", linewidths=1, filtered=True)\n",
    "plt.grid(True, which=\"major\", linestyle=\"dotted\")\n",
    "\n",
    "if PARAM_PLOT_XMIN is not None:\n",
    "    plt.xlim(left=PARAM_PLOT_XMIN)\n",
    "if PARAM_PLOT_XMAX is not None:\n",
    "    plt.xlim(right=PARAM_PLOT_XMAX)\n",
    "if PARAM_PLOT_YMIN is not None:\n",
    "    plt.ylim(bottom=PARAM_PLOT_YMIN)\n",
    "if PARAM_PLOT_YMAX is not None:\n",
    "    plt.ylim(top=PARAM_PLOT_YMAX)\n",
    "\n",
    "plt.xlabel(PARAM_PLOT_XLABEL)\n",
    "plt.ylabel(PARAM_PLOT_YLABEL)\n",
    "\n",
    "plt.axvline(search_overall_metric_value, color=\"black\", linewidth=0.75)\n",
    "plt.axhline(holdout_generalizability_overall_metric_value, color=\"black\", linewidth=0.75)\n",
    "\n",
    "plt.savefig(f\"{STAGE_OUTPUT_PATH}/{PARAM_PLOT_BASENAME}.pdf\")\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "subroc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
