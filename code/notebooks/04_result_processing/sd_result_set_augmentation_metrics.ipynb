{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9ba954f34980343",
   "metadata": {},
   "source": [
    "# Augment a Subgroup Discovery Result Set with Metric Values for each Subgroup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71606acbcb933a6",
   "metadata": {},
   "source": [
    "## Default Values for Papermill Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T15:07:57.663627Z",
     "start_time": "2024-07-03T15:07:57.646605Z"
    },
    "collapsed": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "PARAM_METRICS = [\"average_ranking_loss\"]\n",
    "PARAM_RESULT_SET_PATH = \"../outputs/merged_result_set.csv\"\n",
    "PARAM_AUGMENTATION_RESULT_FILENAME = \"metrics_augmented_result_set.csv\"\n",
    "PARAM_DATA_IN_PATH = \"../../data\"\n",
    "\n",
    "PARAM_DATASET_NAME = \"OpenML Adult\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4594c4db8918611c",
   "metadata": {},
   "source": [
    "## Import and Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "89d8866c280c829a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T15:07:59.205904Z",
     "start_time": "2024-07-03T15:07:57.667604Z"
    }
   },
   "outputs": [],
   "source": [
    "from subroc.datasets.metadata import to_DatasetName\n",
    "from subroc.datasets.reader import DatasetReader, DatasetStage\n",
    "from subroc.quality_functions.sklearn_metrics import get_soft_classification_metric_qf_dict\n",
    "from subroc.quality_functions.base_qf import OptimizationMode\n",
    "from subroc.quality_functions.custom_metrics import get_custom_metric_qf_dict\n",
    "from subroc.preconditions import constraint_for_precondition_disjunction\n",
    "from subroc.util import create_subgroup\n",
    "from subroc import util\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# fill environment variables into params\n",
    "PARAM_RESULT_SET_PATH = util.prepend_experiment_output_path(PARAM_RESULT_SET_PATH)\n",
    "PARAM_DATA_IN_PATH = util.prepend_experiment_output_path(PARAM_DATA_IN_PATH)\n",
    "\n",
    "# get environment variables\n",
    "STAGE_OUTPUT_PATH = os.environ.get(\"STAGE_OUTPUT_PATH\", \"../outputs\")\n",
    "\n",
    "# Dataset\n",
    "dataset_reader = DatasetReader(PARAM_DATA_IN_PATH)\n",
    "\n",
    "DATA_OUT_PATH = f\"{STAGE_OUTPUT_PATH}/data/processed\"\n",
    "if not os.path.exists(DATA_OUT_PATH):\n",
    "    os.makedirs(DATA_OUT_PATH)\n",
    "\n",
    "DATASET_NAME = to_DatasetName(PARAM_DATASET_NAME)\n",
    "\n",
    "if DATASET_NAME is None:\n",
    "    print(f\"dataset name '{PARAM_DATASET_NAME}' not supported.\")\n",
    "\n",
    "DATASET_STAGE = DatasetStage.PROCESSED_MODEL_PREDICTED\n",
    "\n",
    "# interestingness measures to take the metrics from\n",
    "qfs_dict = {\n",
    "    \"average_ranking_loss\": get_custom_metric_qf_dict(\n",
    "        OptimizationMode.Maximal,\n",
    "    )[\"average_ranking_loss\"],\n",
    "    \"sklearn.metrics.roc_auc_score\": get_soft_classification_metric_qf_dict(\n",
    "        OptimizationMode.Maximal,\n",
    "    )[\"sklearn.metrics.roc_auc_score\"],\n",
    "    \"prc_auc_score\": get_custom_metric_qf_dict(\n",
    "        OptimizationMode.Maximal,\n",
    "    )[\"prc_auc_score\"],\n",
    "}\n",
    "metrics = [qfs_dict[param_metric].metric for param_metric in PARAM_METRICS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffb5e70df90a9fe",
   "metadata": {},
   "source": [
    "## Read and Sort the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b22a92be02d03c97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T15:08:00.461204Z",
     "start_time": "2024-07-03T15:07:59.208212Z"
    }
   },
   "outputs": [],
   "source": [
    "# read data\n",
    "(train_data, test_data), dataset_meta = dataset_reader.read_dataset(DATASET_NAME, DATASET_STAGE)\n",
    "\n",
    "# sort data and set up some datastructures to access sorted data\n",
    "dataset_sorted_by_score = test_data.sort_values(dataset_meta.score_name)\n",
    "scores_sorted = dataset_sorted_by_score.loc[:, dataset_meta.score_name]\n",
    "gt_sorted_by_score = dataset_sorted_by_score.loc[:, dataset_meta.gt_name]\n",
    "sorted_to_original_index = [index for index, _ in dataset_sorted_by_score.iterrows()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f60666",
   "metadata": {},
   "source": [
    "## Create the Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e96262",
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints_per_metric = []\n",
    "\n",
    "for param_metric in PARAM_METRICS:\n",
    "    qf = qfs_dict[param_metric]\n",
    "    metric_constraints = []\n",
    "\n",
    "    if hasattr(qf, \"preconditions\"):\n",
    "        for precondition_disjunction in qf.preconditions:\n",
    "            metric_constraints.append(\n",
    "                constraint_for_precondition_disjunction(precondition_disjunction, test_data, dataset_meta))\n",
    "    \n",
    "    constraints_per_metric.append(metric_constraints)\n",
    "\n",
    "constraints_per_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333c304ae30043d3",
   "metadata": {},
   "source": [
    "## Read the Result Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "72fcb491546cd7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T15:08:00.476746Z",
     "start_time": "2024-07-03T15:08:00.463295Z"
    }
   },
   "outputs": [],
   "source": [
    "result_set = pd.read_csv(f\"{PARAM_RESULT_SET_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5242294817af058",
   "metadata": {},
   "source": [
    "## Augment the Result Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "df01584bb5c28272",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T15:08:00.538797Z",
     "start_time": "2024-07-03T15:08:00.479723Z"
    }
   },
   "outputs": [],
   "source": [
    "original_columns = result_set.columns.values.tolist()\n",
    "metrics_augmented_result_set = pd.DataFrame(columns=(original_columns + PARAM_METRICS))\n",
    "\n",
    "for i, result in enumerate(result_set.itertuples()):\n",
    "    # recreate the pysubgroup object for the subgroup with a representation for the dataset\n",
    "    sel_conjunction = util.from_str_Conjunction(result.pattern)\n",
    "    subgroup = create_subgroup(test_data, sel_conjunction.selectors)\n",
    "    \n",
    "    # get true and predicted labels for subgroup cover\n",
    "    sorted_subgroup_representation = \\\n",
    "        [subgroup.representation[original_index] for original_index in sorted_to_original_index]\n",
    "    sorted_subgroup_y_true = gt_sorted_by_score[sorted_subgroup_representation].to_numpy()\n",
    "    sorted_subgroup_y_pred = scores_sorted[sorted_subgroup_representation].to_numpy()\n",
    "    \n",
    "    # compute the metric values\n",
    "    metric_values = []\n",
    "    for metric, constraints in zip(metrics, constraints_per_metric):\n",
    "        if all([constraint.is_satisfied(subgroup, data=test_data) for constraint in constraints]):\n",
    "            metric_values.append(metric(sorted_subgroup_y_true, sorted_subgroup_y_pred))\n",
    "        else:\n",
    "            metric_values.append(np.nan)\n",
    "    \n",
    "    # append the augmented instance to metrics_augmented_result_set\n",
    "    metrics_augmented_result_set.loc[i] = [result[i+1] for i in range(len(original_columns))] + metric_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea26059f570d579",
   "metadata": {},
   "source": [
    "## Write the Augmentation Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "62d1192219060f6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T15:08:00.554713Z",
     "start_time": "2024-07-03T15:08:00.542676Z"
    }
   },
   "outputs": [],
   "source": [
    "metrics_augmented_result_set.to_csv(f\"{STAGE_OUTPUT_PATH}/{PARAM_AUGMENTATION_RESULT_FILENAME}\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
