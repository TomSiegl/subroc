{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Subgroup p-values based on Random Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Values for Papermill Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "PARAM_RESULT_SET_PATH = \"../outputs/sd_result_set_average_ranking_loss.csv\"\n",
    "PARAM_QF_PATH = \"../outputs/interestingness_measure.pickle\"\n",
    "PARAM_QF_OUTPUT_BASENAME = \"p_value_augmentation_interestingness_measure\"\n",
    "PARAM_FILTERING_RESULT_FILENAME = \"p_value_augmented_result_set.csv\"\n",
    "PARAM_DATA_IN_PATH = \"../../data\"\n",
    "PARAM_MODELS_IN_PATH = \"../../models\"\n",
    "\n",
    "PARAM_DATASET_NAME = \"OpenML Adult\"\n",
    "PARAM_DATASET_STAGE = None\n",
    "PARAM_MODEL_NAME = \"sklearn_gaussian_nb_adult_4_splits\"\n",
    "PARAM_NUM_RANDOM_SAMPLES = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subroc.datasets.metadata import to_DatasetName\n",
    "from subroc.datasets.reader import DatasetReader, DatasetStage, meta_dict\n",
    "from subroc.model_serialization import deserialize\n",
    "from subroc.quality_functions.base_qf import PredictionType\n",
    "from subroc.quality_functions.soft_classifier_target import SoftClassifierTarget\n",
    "from subroc import util\n",
    "\n",
    "import pysubgroup as ps\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# fill environment variables into params\n",
    "PARAM_RESULT_SET_PATH = util.prepend_experiment_output_path(PARAM_RESULT_SET_PATH)\n",
    "PARAM_QF_PATH = util.prepend_experiment_output_path(PARAM_QF_PATH)\n",
    "PARAM_DATA_IN_PATH = util.prepend_experiment_output_path(PARAM_DATA_IN_PATH)\n",
    "PARAM_MODELS_IN_PATH = util.prepend_experiment_output_path(PARAM_MODELS_IN_PATH)\n",
    "\n",
    "# get environment variables\n",
    "STAGE_OUTPUT_PATH = os.environ.get(\"STAGE_OUTPUT_PATH\", \"../outputs\")\n",
    "\n",
    "# Dataset\n",
    "dataset_reader = DatasetReader(PARAM_DATA_IN_PATH)\n",
    "\n",
    "DATA_OUT_PATH = f\"{STAGE_OUTPUT_PATH}/data/processed\"\n",
    "if not os.path.exists(DATA_OUT_PATH):\n",
    "    os.makedirs(DATA_OUT_PATH)\n",
    "\n",
    "DATASET_NAME = to_DatasetName(PARAM_DATASET_NAME)\n",
    "\n",
    "if DATASET_NAME is None:\n",
    "    print(f\"dataset name '{PARAM_DATASET_NAME}' not supported.\")\n",
    "\n",
    "if PARAM_DATASET_STAGE is None:\n",
    "    DATASET_STAGE = DatasetStage.PROCESSED_MODEL_READY\n",
    "else:\n",
    "    DATASET_STAGE = DatasetStage(PARAM_DATASET_STAGE)\n",
    "\n",
    "# Model\n",
    "model = deserialize(PARAM_MODELS_IN_PATH, PARAM_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get and Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data and preprocess it for the model\n",
    "dataset_meta = meta_dict[DATASET_NAME]\n",
    "\n",
    "# prepare classification predictions\n",
    "dataset_meta.prediction_type = PredictionType.CLASSIFICATION_SOFT\n",
    "\n",
    "data = None\n",
    "if DATASET_STAGE == DatasetStage.PROCESSED_MODEL_READY:\n",
    "    data = dataset_reader._read_processed(dataset_meta, \"model_ready_holdout_significance.csv\", \",\")\n",
    "\n",
    "    data_x = data.loc[:, data.columns != dataset_meta.gt_name]\n",
    "    data[dataset_meta.score_name] = model.predict(data_x)\n",
    "\n",
    "    # save data with predictions\n",
    "    out_path = DATA_OUT_PATH + \"/\" + dataset_meta.dataset_dir\n",
    "    if not os.path.exists(out_path):\n",
    "        os.mkdir(out_path)\n",
    "\n",
    "    data.to_csv(out_path + \"/\" + \"model_predicted_holdout_significance.csv\", index=False)\n",
    "elif DATASET_STAGE == DatasetStage.PROCESSED_MODEL_PREDICTED:\n",
    "    data = dataset_reader._read_processed(dataset_meta, \"model_predicted_holdout_significance.csv\", \",\")\n",
    "elif DATASET_STAGE == DatasetStage.PROCESSED_PERMUTED_MODEL_PREDICTED:\n",
    "    data = dataset_reader._read_processed(dataset_meta, \"permuted_model_predicted_holdout_significance.csv\", \",\")\n",
    "\n",
    "# sd objects\n",
    "target = SoftClassifierTarget(dataset_meta.gt_name, dataset_meta.score_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the Result Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    result_set = pd.read_csv(f\"{PARAM_RESULT_SET_PATH}\")\n",
    "except pd.errors.EmptyDataError:\n",
    "    result_set = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Configure the Interestingness Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PARAM_QF_PATH, \"rb\") as qf_file:\n",
    "    QF = pickle.load(qf_file)\n",
    "\n",
    "if isinstance(QF, ps.GeneralizationAwareQF):\n",
    "    QF = QF.qf\n",
    "\n",
    "QF.calculate_constant_statistics(data, target)\n",
    "\n",
    "# Disable any significance-related changes to the qf value\n",
    "QF.subgroup_size_weight = 0\n",
    "QF.subgroup_class_balance_weight = 0\n",
    "QF.random_sampling_p_value_factor = False\n",
    "QF.random_sampling_normalization = False\n",
    "\n",
    "# Configure the QF for computing p-values by random sampling\n",
    "QF.num_random_samples = PARAM_NUM_RANDOM_SAMPLES\n",
    "QF.random_sampling_distributions = {}\n",
    "\n",
    "# update the representation of the qf-specific constraints if necessary\n",
    "if hasattr(QF, \"constraints\"):\n",
    "    for constraint in QF.constraints:\n",
    "        if hasattr(constraint, \"update\"):\n",
    "            constraint.update(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment the Result Set (Compute p-values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_columns = result_set.columns.values.tolist()\n",
    "augmented_result_set = pd.DataFrame(columns=original_columns + [\"p-value\", \"filtering interestingness\"])\n",
    "\n",
    "all_patterns = [result.pattern for result in result_set.itertuples()]\n",
    "\n",
    "for i, result in enumerate(result_set.itertuples()):\n",
    "    print(result.pattern)\n",
    "\n",
    "    # recreate the pysubgroup object for the subgroup with a representation for the dataset\n",
    "    sel_conjunction = util.from_str_Conjunction(result.pattern)\n",
    "    subgroup = util.create_subgroup(data, sel_conjunction.selectors)\n",
    "    \n",
    "    statistics = QF.calculate_statistics(subgroup, target, data)\n",
    "\n",
    "    # shortcut for when qf constraints are not met\n",
    "    if not ps.constraints_satisfied(\n",
    "            QF.constraints,\n",
    "            subgroup,\n",
    "            statistics,\n",
    "            data,\n",
    "    ):\n",
    "        augmented_result_set.loc[i] = [result[i+1] for i in range(len(original_columns))] + [np.nan, np.nan]\n",
    "        continue\n",
    "\n",
    "    # compute the qf value (without significance)\n",
    "    qf_value = QF.evaluate(subgroup, target, data, statistics)\n",
    "\n",
    "    # compute the p-value\n",
    "    subgroup_labels = data.loc[subgroup.representation, target.gt_name]\n",
    "    subgroup_size = len(subgroup_labels)\n",
    "    p_value = QF._compute_random_sampling_p_value(subgroup_size, subgroup_labels, target, data, qf_value)\n",
    "\n",
    "    print(f\"p-value: {p_value}\")\n",
    "    print(f\"filtering interestingness: {qf_value}\")\n",
    "    \n",
    "    # append the augmented instance to metrics_augmented_result_set\n",
    "    augmented_result_set.loc[i] = [result[i+1] for i in range(len(original_columns))] + [p_value, qf_value]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the Augmented Result Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_result_set.to_csv(f\"{STAGE_OUTPUT_PATH}/{PARAM_FILTERING_RESULT_FILENAME}\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Interestingness Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(STAGE_OUTPUT_PATH + \"/\" + PARAM_QF_OUTPUT_BASENAME + \".pickle\", \"wb\") as file:\n",
    "    pickle.dump(QF, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "subroc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
